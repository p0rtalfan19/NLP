# –†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –ø–∞—Ä—Å–µ—Ä–æ–≤ –Ω–æ–≤–æ—Å—Ç–µ–π

## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

### 1. –ü–∞—Ä—Å–µ—Ä Lenta.ru

```python
from lenta_parser import LentaParser

# –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞—Ä—Å–µ—Ä–∞
parser = LentaParser(delay=1.0)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏

# –ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
articles = parser.parse_category('news', max_articles=50)

# –ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
all_articles = parser.parse_all_categories(max_articles_per_category=30)

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
parser.save_articles(articles, "lenta_articles.jsonl")
```

### 2. –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä

```python
from universal_news_parser import UniversalNewsParser

# –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞—Ä—Å–µ—Ä–∞
parser = UniversalNewsParser(delay=1.0)

# –ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Å–∞–π—Ç–∞
articles = parser.parse_site('lenta.ru', max_articles_per_category=20)

# –ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Å–∞–π—Ç–æ–≤
all_articles = parser.parse_all_sites(max_articles_per_category=15)

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
parser.save_articles(articles, "news_articles.jsonl")
```

## üìã –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏

### Lenta.ru
- `russia` - –†–æ—Å—Å–∏—è
- `world` - –ú–∏—Ä
- `economics` - –≠–∫–æ–Ω–æ–º–∏–∫–∞
- `sport` - –°–ø–æ—Ä—Ç
- `culture` - –ö—É–ª—å—Ç—É—Ä–∞
- `media` - –ú–µ–¥–∏–∞
- `science` - –ù–∞—É–∫–∞
- `style` - –°—Ç–∏–ª—å –∂–∏–∑–Ω–∏
- `news` - –ù–æ–≤–æ—Å—Ç–∏

### TASS.ru
- `politics` - –ü–æ–ª–∏—Ç–∏–∫–∞
- `economy` - –≠–∫–æ–Ω–æ–º–∏–∫–∞
- `society` - –û–±—â–µ—Å—Ç–≤–æ
- `world` - –ú–∏—Ä
- `sport` - –°–ø–æ—Ä—Ç
- `culture` - –ö—É–ª—å—Ç—É—Ä–∞
- `science` - –ù–∞—É–∫–∞
- `technology` - –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏

### Meduza.io
- `news` - –ù–æ–≤–æ—Å—Ç–∏
- `feature` - –†–µ–ø–æ—Ä—Ç–∞–∂–∏
- `episodes` - –ü–æ–¥–∫–∞—Å—Ç—ã
- `cards` - –ö–∞—Ä—Ç–æ—á–∫–∏

## üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞

### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–∞—Ä—Å–µ—Ä–∞

```python
parser = LentaParser(
    delay=1.0,  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (—Å–µ–∫—É–Ω–¥—ã)
    base_url="https://lenta.ru"  # –ë–∞–∑–æ–≤—ã–π URL
)
```

### –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞

```python
parser = UniversalNewsParser(delay=1.0)

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤–æ–≥–æ —Å–∞–π—Ç–∞
parser.site_configs['new_site.ru'] = {
    'base_url': 'https://new_site.ru',
    'article_patterns': [r'/news/', r'/articles/'],
    'title_selectors': ['h1.title', '.article-title'],
    'text_selectors': ['.article-text', '.content'],
    'date_selectors': ['.date', 'meta[property="article:published_time"]'],
    'category_selectors': ['.category', 'meta[property="article:section"]'],
    'categories': {
        'news': '/news/',
        'politics': '/politics/'
    }
}
```

## üìä –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö

–ö–∞–∂–¥–∞—è —Å—Ç–∞—Ç—å—è —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–ª–µ–¥—É—é—â–∏–µ –ø–æ–ª—è:

```json
{
    "title": "–ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç–∞—Ç—å–∏",
    "text": "–û—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏",
    "date": "2024-01-15T10:30:00",
    "url": "https://lenta.ru/news/2024/01/15/article/",
    "category": "news",
    "tags": ["—Ç–µ–≥1", "—Ç–µ–≥2"],
    "author": "–ê–≤—Ç–æ—Ä —Å—Ç–∞—Ç—å–∏",
    "source": "lenta.ru",
    "parsed_at": "2024-01-15T12:00:00"
}
```

## üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

–ó–∞–ø—É—Å—Ç–∏—Ç–µ —Ç–µ—Å—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–∞–±–æ—Ç—ã –ø–∞—Ä—Å–µ—Ä–æ–≤:

```bash
python test_lenta_parser.py
```

## ‚ö†Ô∏è –í–∞–∂–Ω—ã–µ –∑–∞–º–µ—á–∞–Ω–∏—è

1. **–°–æ–±–ª—é–¥–∞–π—Ç–µ –∑–∞–¥–µ—Ä–∂–∫–∏**: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∑–∞–¥–µ—Ä–∂–∫–∏ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏, —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—Ç—å —Å–µ—Ä–≤–µ—Ä—ã
2. **–ü—Ä–æ–≤–µ—Ä—è–π—Ç–µ robots.txt**: –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –ø–∞—Ä—Å–∏–Ω–≥ —Ä–∞–∑—Ä–µ—à–µ–Ω –ø—Ä–∞–≤–∏–ª–∞–º–∏ —Å–∞–π—Ç–∞
3. **–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–π—Ç–µ –æ—à–∏–±–∫–∏**: –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ try-catch –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫ —Å–µ—Ç–∏
4. **–°–æ—Ö—Ä–∞–Ω—è–π—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã**: –†–µ–≥—É–ª—è—Ä–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–π—Ç–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

## üîç –û—Ç–ª–∞–¥–∫–∞

### –í–∫–ª—é—á–µ–Ω–∏–µ –ø–æ–¥—Ä–æ–±–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è

```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

### –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ —Å–∞–π—Ç–∞

```python
import requests

def check_site_availability(url):
    try:
        response = requests.get(url, timeout=10)
        return response.status_code == 200
    except:
        return False

# –ü—Ä–æ–≤–µ—Ä–∫–∞
if check_site_availability("https://lenta.ru"):
    print("–°–∞–π—Ç –¥–æ—Å—Ç—É–ø–µ–Ω")
else:
    print("–°–∞–π—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
```

## üìà –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

### –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏

- **–ó–∞–¥–µ—Ä–∂–∫–∞**: 1-2 —Å–µ–∫—É–Ω–¥—ã –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
- **–¢–∞–π–º–∞—É—Ç**: 10 —Å–µ–∫—É–Ω–¥ –Ω–∞ –∑–∞–ø—Ä–æ—Å
- **–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π**: 20-50 –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—é –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
- **–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ—Å—Ç—å**: –ù–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫

### –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è

```python
# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å–µ—Å—Å–∏–∏ –¥–ª—è –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
parser = LentaParser()
parser.session.headers.update({
    'Accept-Encoding': 'gzip, deflate',
    'Connection': 'keep-alive'
})
```

## üö® –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫

```python
try:
    articles = parser.parse_category('news', max_articles=50)
except requests.exceptions.RequestException as e:
    print(f"–û—à–∏–±–∫–∞ —Å–µ—Ç–∏: {e}")
except Exception as e:
    print(f"–û–±—â–∞—è –æ—à–∏–±–∫–∞: {e}")
```

## üìù –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### –°–±–æ—Ä –∫–æ—Ä–ø—É—Å–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞

```python
from lenta_parser import LentaParser

parser = LentaParser(delay=1.0)

# –°–±–æ—Ä —Å—Ç–∞—Ç–µ–π –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
categories = ['news', 'russia', 'world', 'economics']
all_articles = []

for category in categories:
    articles = parser.parse_category(category, max_articles=25)
    all_articles.extend(articles)
    print(f"–°–æ–±—Ä–∞–Ω–æ {len(articles)} —Å—Ç–∞—Ç–µ–π –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ {category}")

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞
parser.save_articles(all_articles, "full_corpus.jsonl")
print(f"–í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ {len(all_articles)} —Å—Ç–∞—Ç–µ–π")
```

### –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –æ—Å–Ω–æ–≤–Ω—ã–º –ø–∞–π–ø–ª–∞–π–Ω–æ–º

```python
from nlp import NLPAnalysisPipeline

# –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞
pipeline = NLPAnalysisPipeline(language='russian')

# –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞
articles = pipeline.collect_news_corpus(
    max_articles=200, 
    use_universal=True,  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä
    save_to_file=True
)

# –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞
processed = pipeline.preprocess_corpus()
analysis = pipeline.analyze_tokenization()
```

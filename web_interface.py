import os
os.environ['TCL_LIBRARY'] = "C:/Program Files/Python313/tcl/tcl8.6"
os.environ['TK_LIBRARY'] = "C:/Program Files/Python313/tcl/tk8.6"
import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import json
import io
from datetime import datetime
import logging

# –ò–º–ø–æ—Ä—Ç –Ω–∞—à–∏—Ö –º–æ–¥—É–ª–µ–π
from kommersant_parser import KommersantParser
from text_cleaner import TextCleaner
from universal_preprocessor import UniversalPreprocessor, PreprocessingConfig
from tokenization_analysis import TokenizationAnalyzer
from subword_models import SubwordModelTrainer

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(
    page_title="–ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–æ–≤ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è",
    page_icon="üìù",
    layout="wide",
    initial_sidebar_state="expanded"
)

class TextAnalysisWebApp:
    """–í–µ–±-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–æ–≤ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏"""
    
    def __init__(self):
        self.articles = []
        self.processed_articles = []
        self.analysis_results = {}
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self.kommersant_parser = KommersantParser()
        self.text_cleaner = TextCleaner()
        self.preprocessor = UniversalPreprocessor()
        self.tokenization_analyzer = TokenizationAnalyzer()
        self.subword_trainer = SubwordModelTrainer()
    
    def load_sample_data(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö"""
        sample_texts = [
            "–†–æ—Å—Å–∏—è –∏ –ö–∏—Ç–∞–π –ø–æ–¥–ø–∏—Å–∞–ª–∏ —Å–æ–≥–ª–∞—à–µ–Ω–∏–µ –æ —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–µ –≤ –æ–±–ª–∞—Å—Ç–∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π.",
            "–í –ú–æ—Å–∫–≤–µ —Å–æ—Å—Ç–æ—è–ª–∞—Å—å –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—è –ø–æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–º—É –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É.",
            "–£—á–µ–Ω—ã–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.",
            "–ö–æ–º–ø–∞–Ω–∏—è –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∞ –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö.",
            "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–∑—É—á–∞—é—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π."
        ]
        
        self.articles = [
            {
                'title': f"–ù–æ–≤–æ—Å—Ç—å {i+1}",
                'text': text,
                'date': datetime.now().isoformat(),
                'url': f"https://example.com/news/{i+1}",
                'category': 'technology',
                'source': 'sample'
            }
            for i, text in enumerate(sample_texts)
        ]
        
        return self.articles
    
    def parse_kommersant_news(self, start_id: int, end_id: int, max_articles: int = 100):
        """–ü–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–µ–π —Å –ö–æ–º–º–µ—Ä—Å–∞–Ω—Ç–∞"""
        with st.spinner("–ü–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–µ–π —Å –ö–æ–º–º–µ—Ä—Å–∞–Ω—Ç–∞..."):
            try:
                self.articles = self.kommersant_parser.parse_article_range(start_id, end_id, max_articles)
                return self.articles
            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ: {e}")
                return []
    
    def preprocess_articles(self, config: PreprocessingConfig):
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç–µ–π"""
        if not self.articles:
            return []
        
        self.preprocessor = UniversalPreprocessor(config)
        self.processed_articles = self.preprocessor.batch_preprocess(self.articles)
        return self.processed_articles
    
    def analyze_tokenization(self, texts: list):
        """–ê–Ω–∞–ª–∏–∑ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏"""
        if not texts:
            return {}
        
        return self.tokenization_analyzer.analyze_corpus(texts)
    
    def train_subword_models(self, texts: list, vocab_sizes: list):
        """–û–±—É—á–µ–Ω–∏–µ –ø–æ–¥—Å–ª–æ–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        if not texts:
            return {}
        
        return self.subword_trainer.train_all_models(texts, vocab_sizes)

def main():
    st.title("üìù –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–æ–≤ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è")
    st.markdown("---")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
    if 'app' not in st.session_state:
        st.session_state.app = TextAnalysisWebApp()
    
    app = st.session_state.app
    
    # –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å
    with st.sidebar:
        st.header("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        st.subheader("üìÅ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
        data_source = st.selectbox(
            "–ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö",
            ["–ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö", "–ö–æ–º–º–µ—Ä—Å–∞–Ω—Ç—ä", "–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª"]
        )
        
        if data_source == "–ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö":
            if st.button("üîÑ –ó–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–∏–º–µ—Ä—ã"):
                app.articles = app.load_sample_data()
                st.success(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(app.articles)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        
        elif data_source == "–ö–æ–º–º–µ—Ä—Å–∞–Ω—Ç—ä":
            col1, col2 = st.columns(2)
            with col1:
                start_id = st.number_input("–ù–∞—á–∞–ª—å–Ω—ã–π ID", value=8050000, min_value=1)
            with col2:
                end_id = st.number_input("–ö–æ–Ω–µ—á–Ω—ã–π ID", value=8059999, min_value=1)
            max_articles = st.slider("–ú–∞–∫—Å–∏–º—É–º —Å—Ç–∞—Ç–µ–π", 10, 500, 100)
            if st.button("üîÑ –ü–∞—Ä—Å–∏—Ç—å –ö–æ–º–º–µ—Ä—Å–∞–Ω—Ç—ä"):
                app.articles = app.parse_kommersant_news(start_id, end_id, max_articles)
                if app.articles:
                    st.success(f"–°–ø–∞—Ä—Å–µ–Ω–æ {len(app.articles)} —Å—Ç–∞—Ç–µ–π")
        
        elif data_source == "–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª":
            uploaded_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ JSONL —Ñ–∞–π–ª", type=['jsonl', 'json'])
            if uploaded_file:
                try:
                    content = uploaded_file.read().decode('utf-8')
                    articles = []
                    for line in content.strip().split('\n'):
                        if line:
                            articles.append(json.loads(line))
                    app.articles = articles
                    st.success(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(articles)} —Å—Ç–∞—Ç–µ–π")
                except Exception as e:
                    st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ñ–∞–π–ª–∞: {e}")
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
        st.subheader("üîß –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞")
        
        with st.expander("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏"):
            replace_numbers = st.checkbox("–ó–∞–º–µ–Ω—è—Ç—å —á–∏—Å–ª–∞", value=True)
            replace_urls = st.checkbox("–ó–∞–º–µ–Ω—è—Ç—å URL", value=True)
            replace_emails = st.checkbox("–ó–∞–º–µ–Ω—è—Ç—å email", value=True)
            replace_phones = st.checkbox("–ó–∞–º–µ–Ω—è—Ç—å —Ç–µ–ª–µ—Ñ–æ–Ω—ã", value=True)
            replace_dates = st.checkbox("–ó–∞–º–µ–Ω—è—Ç—å –¥–∞—Ç—ã", value=True)
            replace_times = st.checkbox("–ó–∞–º–µ–Ω—è—Ç—å –≤—Ä–µ–º—è", value=True)
            replace_currencies = st.checkbox("–ó–∞–º–µ–Ω—è—Ç—å –≤–∞–ª—é—Ç—ã", value=True)
            normalize_punctuation = st.checkbox("–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é", value=True)
            normalize_quotes = st.checkbox("–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –∫–∞–≤—ã—á–∫–∏", value=True)
            normalize_dashes = st.checkbox("–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ç–∏—Ä–µ", value=True)
            normalize_spaces = st.checkbox("–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø—Ä–æ–±–µ–ª—ã", value=True)
            expand_abbreviations = st.checkbox("–†–∞—Å—à–∏—Ñ—Ä–æ–≤—ã–≤–∞—Ç—å —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è", value=True)
            to_lowercase = st.checkbox("–ü—Ä–∏–≤–æ–¥–∏—Ç—å –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É", value=False)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∞–Ω–∞–ª–∏–∑–∞
        st.subheader("üìä –ê–Ω–∞–ª–∏–∑")
        
        tokenization_methods = st.multiselect(
            "–ú–µ—Ç–æ–¥—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏",
            ["naive", "regex", "nltk", "spacy", "razdel"],
            default=["naive", "nltk", "razdel"]
        )
        
        normalization_methods = st.multiselect(
            "–ú–µ—Ç–æ–¥—ã –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏",
            ["porter_stem", "snowball_stem", "spacy_lemma", "pymorphy_lemma"],
            default=["porter_stem", "snowball_stem"]
        )
        
        vocab_sizes = st.multiselect(
            "–†–∞–∑–º–µ—Ä—ã —Å–ª–æ–≤–∞—Ä—è –¥–ª—è –ø–æ–¥—Å–ª–æ–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π",
            [4000, 8000, 16000, 32000],
            default=[8000, 16000]
        )
    
    # –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
    if not app.articles:
        st.info("üëà –ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ –≤ –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏ –¥–ª—è –Ω–∞—á–∞–ª–∞ –∞–Ω–∞–ª–∏–∑–∞")
    else:
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞–Ω–Ω—ã—Ö
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("–°—Ç–∞—Ç–µ–π", len(app.articles))
        with col2:
            total_words = sum(len(article['text'].split()) for article in app.articles)
            st.metric("–°–ª–æ–≤", f"{total_words:,}")
        with col3:
            avg_words = total_words // len(app.articles) if app.articles else 0
            st.metric("–°–ª–æ–≤ –Ω–∞ —Å—Ç–∞—Ç—å—é", avg_words)
        with col4:
            categories = set(article.get('category', 'unknown') for article in app.articles)
            st.metric("–ö–∞—Ç–µ–≥–æ—Ä–∏–π", len(categories))
        
        # –í–∫–ª–∞–¥–∫–∏
        tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
            "üìÑ –î–∞–Ω–Ω—ã–µ", "üîß –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞", "‚úÇÔ∏è –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è", 
            "üß© –ü–æ–¥—Å–ª–æ–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏", "üìä –ê–Ω–∞–ª–∏–∑", "üìà –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è"
        ])
        
        with tab1:
            st.header("–ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
            
            # –ü—Ä–æ—Å–º–æ—Ç—Ä —Å—Ç–∞—Ç–µ–π
            if st.checkbox("–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç—å–∏"):
                for i, article in enumerate(app.articles[:5]):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5
                    with st.expander(f"–°—Ç–∞—Ç—å—è {i+1}: {article['title'][:50]}..."):
                        st.write(f"**–ó–∞–≥–æ–ª–æ–≤–æ–∫:** {article['title']}")
                        st.write(f"**–ö–∞—Ç–µ–≥–æ—Ä–∏—è:** {article.get('category', '–ù–µ —É–∫–∞–∑–∞–Ω–∞')}")
                        st.write(f"**–î–∞—Ç–∞:** {article.get('date', '–ù–µ —É–∫–∞–∑–∞–Ω–∞')}")
                        st.write(f"**–¢–µ–∫—Å—Ç:** {article['text'][:500]}...")
                        if article.get('url'):
                            st.write(f"**URL:** {article['url']}")
        
        with tab2:
            st.header("–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞")
            
            if st.button("üîÑ –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ç–µ–∫—Å—Ç—ã"):
                # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
                config = PreprocessingConfig(
                    replace_numbers=replace_numbers,
                    replace_urls=replace_urls,
                    replace_emails=replace_emails,
                    replace_phones=replace_phones,
                    replace_dates=replace_dates,
                    replace_times=replace_times,
                    replace_currencies=replace_currencies,
                    normalize_punctuation=normalize_punctuation,
                    normalize_quotes=normalize_quotes,
                    normalize_dashes=normalize_dashes,
                    normalize_spaces=normalize_spaces,
                    expand_abbreviations=expand_abbreviations,
                    to_lowercase=to_lowercase
                )
                
                with st.spinner("–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤..."):
                    app.processed_articles = app.preprocess_articles(config)
                
                if app.processed_articles:
                    st.success(f"–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(app.processed_articles)} —Å—Ç–∞—Ç–µ–π")
                    
                    # –ü–æ–∫–∞–∑–∞—Ç—å –ø—Ä–∏–º–µ—Ä
                    if app.processed_articles:
                        st.subheader("–ü—Ä–∏–º–µ—Ä –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏")
                        original = app.articles[0]['text'][:300]
                        processed = app.processed_articles[0]['text'][:300]
                        
                        col1, col2 = st.columns(2)
                        with col1:
                            st.write("**–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç:**")
                            st.text(original)
                        with col2:
                            st.write("**–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:**")
                            st.text(processed)
        
        with tab3:
            st.header("–ê–Ω–∞–ª–∏–∑ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏")
            
            if st.button("üîÑ –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é"):
                texts = [article['text'] for article in app.articles]
                
                with st.spinner("–ê–Ω–∞–ª–∏–∑ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏..."):
                    app.analysis_results = app.analyze_tokenization(texts)
                
                if app.analysis_results:
                    st.success("–ê–Ω–∞–ª–∏–∑ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∑–∞–≤–µ—Ä—à–µ–Ω")
                    
                    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
                    if 'comparison' in app.analysis_results:
                        comparison_data = []
                        for method, metrics in app.analysis_results['comparison'].items():
                            comparison_data.append({
                                '–ú–µ—Ç–æ–¥': method,
                                '–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è (train)': metrics['train_vocab_size'],
                                '–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è (test)': metrics['test_vocab_size'],
                                'OOV Rate': f"{metrics['oov_rate']:.4f}",
                                '–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ': f"{metrics['semantic_similarity']:.4f}",
                                '–í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (—Å)': f"{metrics['train_processing_time']:.4f}"
                            })
                        
                        df = pd.DataFrame(comparison_data)
                        st.dataframe(df, use_container_width=True)
        
        with tab4:
            st.header("–ü–æ–¥—Å–ª–æ–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏")
            
            if st.button("üîÑ –û–±—É—á–∏—Ç—å –ø–æ–¥—Å–ª–æ–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏"):
                texts = [article['text'] for article in app.articles]
                
                with st.spinner("–û–±—É—á–µ–Ω–∏–µ –ø–æ–¥—Å–ª–æ–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π..."):
                    subword_results = app.train_subword_models(texts, vocab_sizes)
                
                if subword_results and 'evaluation_results' in subword_results:
                    st.success("–û–±—É—á–µ–Ω–∏–µ –ø–æ–¥—Å–ª–æ–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
                    
                    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏
                    eval_data = []
                    for model_name, metrics in subword_results['evaluation_results'].items():
                        eval_data.append({
                            '–ú–æ–¥–µ–ª—å': model_name,
                            '–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏–∏': f"{metrics['fragmentation_rate']:.4f}",
                            '–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∂–∞—Ç–∏—è': f"{metrics['compression_ratio']:.4f}",
                            '–í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (—Å)': f"{metrics['avg_processing_time']:.4f}",
                            '–¢–æ–∫–µ–Ω–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É': f"{metrics['tokens_per_second']:.2f}"
                        })
                    
                    df = pd.DataFrame(eval_data)
                    st.dataframe(df, use_container_width=True)
        
        with tab5:
            st.header("–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑")
            
            if app.articles:
                # –ê–Ω–∞–ª–∏–∑ –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–æ–≤
                text_lengths = [len(article['text'].split()) for article in app.articles]
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.subheader("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–æ–≤")
                    fig = px.histogram(
                        x=text_lengths,
                        nbins=20,
                        title="–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–ª–æ–≤ –≤ —Å—Ç–∞—Ç—å—è—Ö"
                    )
                    fig.update_layout(
                        xaxis_title="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤",
                        yaxis_title="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π"
                    )
                    st.plotly_chart(fig, use_container_width=True)
                
                with col2:
                    st.subheader("–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–æ–≤")
                    stats_data = {
                        '–ú–µ—Ç—Ä–∏–∫–∞': ['–°—Ä–µ–¥–Ω–µ–µ', '–ú–µ–¥–∏–∞–Ω–∞', '–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ', '–ú–∏–Ω–∏–º—É–º', '–ú–∞–∫—Å–∏–º—É–º'],
                        '–ó–Ω–∞—á–µ–Ω–∏–µ': [
                            f"{np.mean(text_lengths):.1f}",
                            f"{np.median(text_lengths):.1f}",
                            f"{np.std(text_lengths):.1f}",
                            f"{min(text_lengths)}",
                            f"{max(text_lengths)}"
                        ]
                    }
                    st.dataframe(pd.DataFrame(stats_data), use_container_width=True)
                
                # –ê–Ω–∞–ª–∏–∑ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
                if any(article.get('category') for article in app.articles):
                    st.subheader("–ê–Ω–∞–ª–∏–∑ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º")
                    
                    category_stats = {}
                    for article in app.articles:
                        category = article.get('category', 'unknown')
                        if category not in category_stats:
                            category_stats[category] = []
                        category_stats[category].append(len(article['text'].split()))
                    
                    category_data = []
                    for category, lengths in category_stats.items():
                        category_data.append({
                            '–ö–∞—Ç–µ–≥–æ—Ä–∏—è': category,
                            '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π': len(lengths),
                            '–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞': f"{np.mean(lengths):.1f}",
                            '–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤': sum(lengths)
                        })
                    
                    df_categories = pd.DataFrame(category_data)
                    st.dataframe(df_categories, use_container_width=True)
        
        with tab6:
            st.header("–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            
            if app.analysis_results and 'comparison' in app.analysis_results:
                # –ì—Ä–∞—Ñ–∏–∫ OOV Rate
                comparison_data = app.analysis_results['comparison']
                methods = list(comparison_data.keys())
                oov_rates = [comparison_data[method]['oov_rate'] for method in methods]
                
                fig = px.bar(
                    x=methods,
                    y=oov_rates,
                    title="–°—Ä–∞–≤–Ω–µ–Ω–∏–µ OOV Rate –ø–æ –º–µ—Ç–æ–¥–∞–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏"
                )
                fig.update_layout(
                    xaxis_title="–ú–µ—Ç–æ–¥ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏",
                    yaxis_title="OOV Rate"
                )
                st.plotly_chart(fig, use_container_width=True)
                
                # –ì—Ä–∞—Ñ–∏–∫ –≤—Ä–µ–º–µ–Ω–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
                processing_times = [comparison_data[method]['train_processing_time'] for method in methods]
                
                fig2 = px.bar(
                    x=methods,
                    y=processing_times,
                    title="–í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ –º–µ—Ç–æ–¥–∞–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏"
                )
                fig2.update_layout(
                    xaxis_title="–ú–µ—Ç–æ–¥ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏",
                    yaxis_title="–í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (—Å–µ–∫—É–Ω–¥—ã)"
                )
                st.plotly_chart(fig2, use_container_width=True)
        
        # –≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        st.header("üì§ –≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if st.button("üíæ –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ"):
                if app.processed_articles:
                    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ JSONL
                    jsonl_data = '\n'.join(json.dumps(article, ensure_ascii=False) for article in app.processed_articles)
                    st.download_button(
                        label="üìÑ –°–∫–∞—á–∞—Ç—å JSONL",
                        data=jsonl_data,
                        file_name=f"processed_articles_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl",
                        mime="text/plain"
                    )
        
        with col2:
            if st.button("üìä –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞"):
                if app.analysis_results:
                    analysis_json = json.dumps(app.analysis_results, ensure_ascii=False, indent=2, default=str)
                    st.download_button(
                        label="üìà –°–∫–∞—á–∞—Ç—å –∞–Ω–∞–ª–∏–∑",
                        data=analysis_json,
                        file_name=f"analysis_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                        mime="application/json"
                    )
        
        with col3:
            if st.button("üìã –°–æ–∑–¥–∞—Ç—å –æ—Ç—á–µ—Ç"):
                # –°–æ–∑–¥–∞–Ω–∏–µ HTML –æ—Ç—á–µ—Ç–∞
                report_html = f"""
                <!DOCTYPE html>
                <html>
                <head>
                    <title>–û—Ç—á–µ—Ç –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–æ–≤</title>
                    <meta charset="utf-8">
                    <style>
                        body {{ font-family: Arial, sans-serif; margin: 40px; }}
                        h1, h2 {{ color: #333; }}
                        .section {{ margin-bottom: 30px; }}
                        .stats {{ background-color: #f5f5f5; padding: 15px; border-radius: 5px; }}
                        table {{ border-collapse: collapse; width: 100%; }}
                        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
                        th {{ background-color: #f2f2f2; }}
                    </style>
                </head>
                <body>
                    <h1>–û—Ç—á–µ—Ç –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–æ–≤ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏</h1>
                    <p><strong>–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
                    
                    <div class="section">
                        <h2>–û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è</h2>
                        <div class="stats">
                            <p><strong>–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π:</strong> {len(app.articles)}</p>
                            <p><strong>–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤:</strong> {sum(len(article['text'].split()) for article in app.articles):,}</p>
                            <p><strong>–°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –Ω–∞ —Å—Ç–∞—Ç—å—é:</strong> {sum(len(article['text'].split()) for article in app.articles) // len(app.articles) if app.articles else 0:,}</p>
                        </div>
                    </div>
                </body>
                </html>
                """
                
                st.download_button(
                    label="üìÑ –°–∫–∞—á–∞—Ç—å –æ—Ç—á–µ—Ç",
                    data=report_html,
                    file_name=f"text_analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html",
                    mime="text/html"
                )

if __name__ == "__main__":
    main()

#!/usr/bin/env python3
"""
–≠—Ç–∞–ø 4: –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –º–µ—Ç–æ–¥–æ–≤ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
"""
import json
import os
import nltk
from tokenization_analysis import TokenizationAnalyzer

def main():
    nltk.download('punkt_tab')
    print("=" * 60)
    print("–≠–¢–ê–ü 4: –ê–ù–ê–õ–ò–ó –ú–ï–¢–û–î–û–í –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–ò –ò –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–ò")
    print("=" * 60)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    input_file = "kommersant_articles_processed.jsonl"
    if not os.path.exists(input_file):
        print(f"‚ùå –§–∞–π–ª {input_file} –Ω–µ –Ω–∞–π–¥–µ–Ω! –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ —ç—Ç–∞–ø 3.")
        return
    
    print(f"üìÅ –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ {input_file}...")
    articles = []
    with open(input_file, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                articles.append(json.loads(line))
    
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(articles)} —Å—Ç–∞—Ç–µ–π")
    
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤
    texts = [article['text'] for article in articles]
    print(f"üìù –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
    analyzer = TokenizationAnalyzer(language='russian')
    
    print("üîç –ù–∞—á–∏–Ω–∞–µ–º –∞–Ω–∞–ª–∏–∑ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏...")
    
    # –ê–Ω–∞–ª–∏–∑ –∫–æ—Ä–ø—É—Å–∞
    results = analyzer.analyze_corpus(texts, test_size=0.2)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    results_file = "tokenization_analysis_results.json"
    analyzer.save_results(results, results_file)
    
    print(f"‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω")
    print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {results_file}")
    print(f"üìä –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ tokenization_metrics.csv")
    
    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    if 'comparison' in results:
        print(f"\nüìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ—Ç–æ–¥–æ–≤ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏:")
        print(f"{'–ú–µ—Ç–æ–¥':<15} {'OOV Rate':<10} {'–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ':<20} {'–í—Ä–µ–º—è (—Å)':<10}")
        print("-" * 60)
        
        for method, metrics in results['comparison'].items():
            print(f"{method:<15} {metrics['oov_rate']:<10.4f} {metrics['semantic_similarity']:<20.4f} {metrics['train_processing_time']:<10.4f}")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        best_oov = min(results['comparison'].items(), key=lambda x: x[1]['oov_rate'])
        fastest = min(results['comparison'].items(), key=lambda x: x[1]['train_processing_time'])
        
        print(f"\nüèÜ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
        print(f"   –õ—É—á—à–∏–π –ø–æ OOV rate: {best_oov[0]} ({best_oov[1]['oov_rate']:.4f})")
        print(f"   –°–∞–º—ã–π –±—ã—Å—Ç—Ä—ã–π: {fastest[0]} ({fastest[1]['train_processing_time']:.4f}—Å)")
    
    print("\nüéâ –≠—Ç–∞–ø 4 –∑–∞–≤–µ—Ä—à–µ–Ω!")

if __name__ == "__main__":
    main()


{
  "training_results": {
    "bpe_8000": {
      "model_type": "BPE",
      "vocab_size": 8000,
      "min_frequency": 2,
      "training_time": 0.21695756912231445,
      "model_path": "bpe_model_8000.json",
      "test_tokens": [
        "э",
        "то",
        "при",
        "мер",
        "те",
        "к",
        "ста",
        "д",
        "ля",
        "те",
        "сти",
        "ров",
        "ания",
        "b",
        "p",
        "e",
        "мод",
        "ели",
        "."
      ],
      "test_token_count": 19,
      "tokenizer": "Tokenizer(version=\"1.0\", truncation=None, padding=None, added_tokens=[{\"id\":0, \"content\":\"<UNK>\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, ...}, {\"id\":1, \"content\":\"<PAD>\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, ...}, {\"id\":2, \"content\":\"<BOS>\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, ...}, {\"id\":3, \"content\":\"<EOS>\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, ...}], normalizer=Sequence(normalizers=[NFD(), Lowercase()]), pre_tokenizer=Whitespace(), post_processor=None, decoder=None, model=BPE(dropout=None, unk_token=None, continuing_subword_prefix=None, end_of_word_suffix=None, fuse_unk=False, byte_fallback=False, ignore_merges=False, vocab={\"<UNK>\":0, \"<PAD>\":1, \"<BOS>\":2, \"<EOS>\":3, \"(\":4, ...}, merges=[(\"n\", \"u\"), (\"nu\", \"m\"), (\"с\", \"т\"), (\"р\", \"о\"), (\"с\", \"к\"), ...]))"
    },
    "unigram_8000": {
      "model_type": "Unigram",
      "vocab_size": 8000,
      "min_frequency": 2,
      "training_time": 0.14896130561828613,
      "model_path": "unigram_model_8000.json",
      "test_tokens": [
        "эт",
        "о",
        "при",
        "мер",
        "т",
        "е",
        "к",
        "ста",
        "д",
        "л",
        "я",
        "т",
        "е",
        "ст",
        "ирова",
        "ния",
        "u",
        "n",
        "i",
        "g",
        "r",
        "a",
        "m",
        "мо",
        "дел",
        "и",
        "."
      ],
      "test_token_count": 27,
      "tokenizer": "Tokenizer(version=\"1.0\", truncation=None, padding=None, added_tokens=[{\"id\":0, \"content\":\"<UNK>\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, ...}, {\"id\":1, \"content\":\"<PAD>\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, ...}, {\"id\":2, \"content\":\"<BOS>\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, ...}, {\"id\":3, \"content\":\"<EOS>\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, ...}], normalizer=Sequence(normalizers=[NFD(), Lowercase()]), pre_tokenizer=Whitespace(), post_processor=None, decoder=None, model=Unigram(unk_id=None, vocab=[(\"<UNK>\", 0), (\"<PAD>\", 0), (\"<BOS>\", 0), (\"<EOS>\", 0), (\"n\", -2.757410735724191), ...], byte_fallback=False))"
    },
    "sp_bpe_8000": {
      "model_type": "SentencePiece_bpe",
      "vocab_size": 8000,
      "training_time": 0,
      "model_path": "sp_bpe_8000.model",
      "test_tokens": [
        "▁э",
        "то",
        "▁при",
        "мер",
        "▁те",
        "к",
        "ста",
        "▁д",
        "ля",
        "▁т",
        "ест",
        "и",
        "ров",
        "ания",
        "▁",
        "s",
        "en",
        "t",
        "en",
        "c",
        "e",
        "p",
        "i",
        "e",
        "c",
        "e",
        "▁мод",
        "ели",
        "."
      ],
      "test_token_count": 29,
      "tokenizer": "<sentencepiece.SentencePieceProcessor; proxy of <Swig Object of type 'sentencepiece::SentencePieceProcessor *' at 0x000001BB3372A880> >"
    },
    "bpe_16000": {
      "model_type": "BPE",
      "vocab_size": 16000,
      "min_frequency": 2,
      "training_time": 0.19532418251037598,
      "model_path": "bpe_model_16000.json",
      "test_tokens": [
        "э",
        "то",
        "при",
        "мер",
        "те",
        "к",
        "ста",
        "д",
        "ля",
        "те",
        "сти",
        "ров",
        "ания",
        "b",
        "p",
        "e",
        "мод",
        "ели",
        "."
      ],
      "test_token_count": 19,
      "tokenizer": "Tokenizer(version=\"1.0\", truncation=None, padding=None, added_tokens=[{\"id\":0, \"content\":\"<UNK>\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, ...}, {\"id\":1, \"content\":\"<PAD>\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, ...}, {\"id\":2, \"content\":\"<BOS>\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, ...}, {\"id\":3, \"content\":\"<EOS>\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, ...}], normalizer=Sequence(normalizers=[NFD(), Lowercase()]), pre_tokenizer=Whitespace(), post_processor=None, decoder=None, model=BPE(dropout=None, unk_token=None, continuing_subword_prefix=None, end_of_word_suffix=None, fuse_unk=False, byte_fallback=False, ignore_merges=False, vocab={\"<UNK>\":0, \"<PAD>\":1, \"<BOS>\":2, \"<EOS>\":3, \"(\":4, ...}, merges=[(\"n\", \"u\"), (\"nu\", \"m\"), (\"с\", \"т\"), (\"р\", \"о\"), (\"с\", \"к\"), ...]))"
    },
    "unigram_16000": {
      "model_type": "Unigram",
      "vocab_size": 16000,
      "min_frequency": 2,
      "training_time": 0.154282808303833,
      "model_path": "unigram_model_16000.json",
      "test_tokens": [
        "эт",
        "о",
        "при",
        "мер",
        "т",
        "е",
        "к",
        "ста",
        "д",
        "л",
        "я",
        "т",
        "е",
        "ст",
        "ирова",
        "ния",
        "u",
        "n",
        "i",
        "g",
        "r",
        "a",
        "m",
        "мо",
        "дел",
        "и",
        "."
      ],
      "test_token_count": 27,
      "tokenizer": "Tokenizer(version=\"1.0\", truncation=None, padding=None, added_tokens=[{\"id\":0, \"content\":\"<UNK>\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, ...}, {\"id\":1, \"content\":\"<PAD>\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, ...}, {\"id\":2, \"content\":\"<BOS>\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, ...}, {\"id\":3, \"content\":\"<EOS>\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, ...}], normalizer=Sequence(normalizers=[NFD(), Lowercase()]), pre_tokenizer=Whitespace(), post_processor=None, decoder=None, model=Unigram(unk_id=None, vocab=[(\"<UNK>\", 0), (\"<PAD>\", 0), (\"<BOS>\", 0), (\"<EOS>\", 0), (\"n\", -2.757410735724191), ...], byte_fallback=False))"
    },
    "sp_bpe_16000": {
      "model_type": "SentencePiece_bpe",
      "vocab_size": 16000,
      "training_time": 0,
      "model_path": "sp_bpe_16000.model",
      "test_tokens": [
        "▁это",
        "▁при",
        "мер",
        "▁те",
        "к",
        "ста",
        "▁для",
        "▁т",
        "ест",
        "и",
        "ров",
        "ания",
        "▁",
        "s",
        "en",
        "t",
        "en",
        "c",
        "e",
        "p",
        "i",
        "e",
        "c",
        "e",
        "▁мод",
        "ели",
        "."
      ],
      "test_token_count": 27,
      "tokenizer": "<sentencepiece.SentencePieceProcessor; proxy of <Swig Object of type 'sentencepiece::SentencePieceProcessor *' at 0x000001BB336C6E20> >"
    },
    "bpe_32000": {
      "model_type": "BPE",
      "vocab_size": 32000,
      "min_frequency": 2,
      "training_time": 0.1914067268371582,
      "model_path": "bpe_model_32000.json",
      "test_tokens": [
        "э",
        "то",
        "при",
        "мер",
        "те",
        "к",
        "ста",
        "д",
        "ля",
        "те",
        "сти",
        "ров",
        "ания",
        "b",
        "p",
        "e",
        "мод",
        "ели",
        "."
      ],
      "test_token_count": 19,
      "tokenizer": "Tokenizer(version=\"1.0\", truncation=None, padding=None, added_tokens=[{\"id\":0, \"content\":\"<UNK>\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, ...}, {\"id\":1, \"content\":\"<PAD>\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, ...}, {\"id\":2, \"content\":\"<BOS>\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, ...}, {\"id\":3, \"content\":\"<EOS>\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, ...}], normalizer=Sequence(normalizers=[NFD(), Lowercase()]), pre_tokenizer=Whitespace(), post_processor=None, decoder=None, model=BPE(dropout=None, unk_token=None, continuing_subword_prefix=None, end_of_word_suffix=None, fuse_unk=False, byte_fallback=False, ignore_merges=False, vocab={\"<UNK>\":0, \"<PAD>\":1, \"<BOS>\":2, \"<EOS>\":3, \"(\":4, ...}, merges=[(\"n\", \"u\"), (\"nu\", \"m\"), (\"с\", \"т\"), (\"р\", \"о\"), (\"с\", \"к\"), ...]))"
    },
    "unigram_32000": {
      "model_type": "Unigram",
      "vocab_size": 32000,
      "min_frequency": 2,
      "training_time": 0.14630508422851562,
      "model_path": "unigram_model_32000.json",
      "test_tokens": [
        "эт",
        "о",
        "при",
        "мер",
        "т",
        "е",
        "к",
        "ста",
        "д",
        "л",
        "я",
        "т",
        "е",
        "ст",
        "ирова",
        "ния",
        "u",
        "n",
        "i",
        "g",
        "r",
        "a",
        "m",
        "мо",
        "дел",
        "и",
        "."
      ],
      "test_token_count": 27,
      "tokenizer": "Tokenizer(version=\"1.0\", truncation=None, padding=None, added_tokens=[{\"id\":0, \"content\":\"<UNK>\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, ...}, {\"id\":1, \"content\":\"<PAD>\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, ...}, {\"id\":2, \"content\":\"<BOS>\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, ...}, {\"id\":3, \"content\":\"<EOS>\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, ...}], normalizer=Sequence(normalizers=[NFD(), Lowercase()]), pre_tokenizer=Whitespace(), post_processor=None, decoder=None, model=Unigram(unk_id=None, vocab=[(\"<UNK>\", 0), (\"<PAD>\", 0), (\"<BOS>\", 0), (\"<EOS>\", 0), (\"n\", -2.757410735724191), ...], byte_fallback=False))"
    }
  },
  "evaluation_results": {
    "bpe": {
      "model_name": "bpe",
      "total_words": 3883,
      "total_tokens": 7348,
      "fragmented_words": 154,
      "fragmentation_rate": 0.039660056657223795,
      "compression_ratio": 0.5284431137724551,
      "avg_processing_time": 0.00017070293426513672,
      "tokens_per_second": 430455.40087711945
    },
    "unigram": {
      "model_name": "unigram",
      "total_words": 3883,
      "total_tokens": 12846,
      "fragmented_words": 63,
      "fragmentation_rate": 0.016224568632500644,
      "compression_ratio": 0.3022730811147439,
      "avg_processing_time": 0.00022039413452148437,
      "tokens_per_second": 582864.876503678
    },
    "sentencepiece_bpe": {
      "model_name": "sentencepiece_bpe",
      "total_words": 3883,
      "total_tokens": 7004,
      "fragmented_words": 221,
      "fragmentation_rate": 0.05691475663147051,
      "compression_ratio": 0.5543974871501999,
      "avg_processing_time": 0.00014908790588378907,
      "tokens_per_second": 469789.95100108744
    }
  },
  "vocab_sizes": [
    8000,
    16000,
    32000
  ],
  "corpus_size": 2000
}
#!/usr/bin/env python3
"""
–≠—Ç–∞–ø 5: –û–±—É—á–µ–Ω–∏–µ –ø–æ–¥—Å–ª–æ–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
"""
import json
import os
from subword_models import SubwordModelTrainer

def main():
    print("=" * 60)
    print("–≠–¢–ê–ü 5: –û–ë–£–ß–ï–ù–ò–ï –ü–û–î–°–õ–û–í–ù–´–• –ú–û–î–ï–õ–ï–ô –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–ò")
    print("=" * 60)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    input_file = "kommersant_articles_processed.jsonl"
    if not os.path.exists(input_file):
        print(f"‚ùå –§–∞–π–ª {input_file} –Ω–µ –Ω–∞–π–¥–µ–Ω! –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ —ç—Ç–∞–ø 3.")
        return
    
    print(f"üìÅ –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ {input_file}...")
    articles = []
    with open(input_file, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                articles.append(json.loads(line))
    
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(articles)} —Å—Ç–∞—Ç–µ–π")
    
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤
    texts = [article['text'] for article in articles]
    print(f"üìù –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–µ—Ä–∞
    trainer = SubwordModelTrainer(language='russian')
    
    # –†–∞–∑–º–µ—Ä—ã —Å–ª–æ–≤–∞—Ä—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    vocab_sizes = [8000, 16000, 32000]
    print(f"üéØ –†–∞–∑–º–µ—Ä—ã —Å–ª–æ–≤–∞—Ä—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {vocab_sizes}")
    
    print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –ø–æ–¥—Å–ª–æ–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π...")
    print("‚è≥ –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –≤—Ä–µ–º—è...")
    
    # –û–±—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
    results = trainer.train_all_models(texts, vocab_sizes)
    
    print(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
    print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ subword_models_results.json")
    print(f"üìä –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ subword_models_comparison.csv")
    
    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    if 'evaluation_results' in results:
        print(f"\nüìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ –ø–æ–¥—Å–ª–æ–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π:")
        print(f"{'–ú–æ–¥–µ–ª—å':<25} {'–§—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏—è':<15} {'–°–∂–∞—Ç–∏–µ':<10} {'–í—Ä–µ–º—è (—Å)':<10} {'–¢–æ–∫–µ–Ω–æ–≤/—Å':<10}")
        print("-" * 80)
        
        for model_name, metrics in results['evaluation_results'].items():
            print(f"{model_name:<25} {metrics['fragmentation_rate']:<15.4f} {metrics['compression_ratio']:<10.4f} {metrics['avg_processing_time']:<10.4f} {metrics['tokens_per_second']:<10.2f}")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        best_fragmentation = min(results['evaluation_results'].items(), key=lambda x: x[1]['fragmentation_rate'])
        fastest_subword = min(results['evaluation_results'].items(), key=lambda x: x[1]['avg_processing_time'])
        best_compression = max(results['evaluation_results'].items(), key=lambda x: x[1]['compression_ratio'])
        
        print(f"\nüèÜ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
        print(f"   –õ—É—á—à–∞—è –ø–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏–∏: {best_fragmentation[0]} ({best_fragmentation[1]['fragmentation_rate']:.4f})")
        print(f"   –°–∞–º–∞—è –±—ã—Å—Ç—Ä–∞—è: {fastest_subword[0]} ({fastest_subword[1]['avg_processing_time']:.4f}—Å)")
        print(f"   –õ—É—á—à–∞—è –ø–æ —Å–∂–∞—Ç–∏—é: {best_compression[0]} ({best_compression[1]['compression_ratio']:.4f})")
    
    print("\nüéâ –≠—Ç–∞–ø 5 –∑–∞–≤–µ—Ä—à–µ–Ω!")

if __name__ == "__main__":
    main()

